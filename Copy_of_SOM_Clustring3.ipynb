{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "WRwQtbLdF8D7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet34\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from numpy.ma.core import ceil\n",
        "from scipy.spatial import distance #distance calculation\n",
        "from sklearn.preprocessing import MinMaxScaler #normalisation\n",
        "from sklearn.metrics import accuracy_score #scoring\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "rd6J04URiw13"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)), # Resize to 224x224 (height x width)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a04AF3Btiw14",
        "outputId": "e7c736e9-44fd-4e80-943f-0d52e8605df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# loading the train data\n",
        "batch_size = 1\n",
        "\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True )\n",
        "\n",
        "#loading the test data\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform)\n",
        "test_dataloader = DataLoader(test_data,batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnYvjUaDiw15",
        "outputId": "5df77f05-64d1-41e3-e3e6-ebbbc11c2dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zBgVyWjTiw16"
      },
      "source": [
        "## Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_8Pik5kiw1-",
        "outputId": "785195c0-43df-44fc-9b07-dbc367674507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "source": [
        "feature_extractor = resnet34(pretrained=True)\n",
        "num_features = feature_extractor.fc.in_features\n",
        "\n",
        "for param in feature_extractor.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "feature_extractor.fc = nn.Identity()\n",
        "feature_extractor.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c5EfC7Wciw2A"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "zC2xrU5Viw2B"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Manhattan distance\n",
        "def manhattan_distance(x, y):\n",
        "  return distance.cityblock(x,y)\n",
        "\n",
        "# Euclidean distance\n",
        "def euclidean_distance(x, y):\n",
        "    return torch.sqrt(torch.sum((x - y) ** 2, dim=-1))\n",
        "\n",
        "\n",
        "# Best Matching Unit search\n",
        "def bmu_search(data, som, num_rows, num_cols):\n",
        "  winner = [0,0]\n",
        "  # som = som.to(device)\n",
        "  # data = data.to(device)\n",
        "  shortest_distance = 10e6 \n",
        "  for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "      if som[row][col] != None:\n",
        "        \n",
        "        distance = euclidean_distance(som[row][col], data)\n",
        "        if distance < shortest_distance: \n",
        "          shortest_distance = distance\n",
        "          winner = [row,col]\n",
        "  return winner\n",
        "\n",
        "#guassian\n",
        "def distance_func(x):\n",
        "  sig = 2 \n",
        "  return np.exp(-np.power(x , 2.) / (2 * np.power(sig, 2.)))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Optimizer\n",
        "class FactorScheduler:\n",
        "    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.001):\n",
        "        self.factor = factor\n",
        "        self.stop_factor_lr = stop_factor_lr\n",
        "        self.base_lr = base_lr\n",
        "\n",
        "    def __call__(self, num_update):\n",
        "        #self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)\n",
        "        return self.base_lr\n",
        "\n",
        "class CosineScheduler:\n",
        "    def __init__(self, max_update, base_lr=0.01, final_lr=0,\n",
        "               warmup_steps=0, warmup_begin_lr=0):\n",
        "        self.base_lr_orig = base_lr\n",
        "        self.max_update = max_update\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.warmup_begin_lr = warmup_begin_lr\n",
        "        self.max_steps = self.max_update - self.warmup_steps\n",
        "\n",
        "    def get_warmup_lr(self, epoch):\n",
        "        increase = (self.base_lr_orig - self.warmup_begin_lr) \\\n",
        "                       * float(epoch) / float(self.warmup_steps)\n",
        "        return self.warmup_begin_lr + increase\n",
        "\n",
        "    def __call__(self, epoch):\n",
        "        if epoch < self.warmup_steps:\n",
        "            return self.get_warmup_lr(epoch)\n",
        "        if epoch <= self.max_update:\n",
        "            self.base_lr = self.final_lr + (\n",
        "                self.base_lr_orig - self.final_lr) * (1 + math.cos(\n",
        "                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2\n",
        "        return self.base_lr\n",
        "        #return self.base_lr_orig\n",
        "\n",
        "# Learning rate and neighbourhood range calculation\n",
        "def neighborhood_optimizer(step, max_steps, max_m_distance):\n",
        "  coefficient = 1.0 - (np.float64(step)/max_steps)\n",
        "  # neighbourhood_range = ceil(coefficient * max_m_distance)\n",
        "  neighbourhood_range = max_m_distance\n",
        "  return neighbourhood_range"
      ],
      "metadata": {
        "id": "4C6zs20r-o8K"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3pPHMXnXiw2C"
      },
      "source": [
        "### Feature Extracting Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "VqVA79bTiw2D"
      },
      "outputs": [],
      "source": [
        "# y_data_list = []\n",
        "# data_list = []\n",
        "# for x_train, y_train in train_dataloader:\n",
        "#   x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "\n",
        "#   features = feature_extractor(x_train)\n",
        "#   features = minmax_scaler(features.cpu().numpy())\n",
        "#   features = torch.from_numpy(features)\n",
        "#   data_list.append(features)\n",
        "\n",
        "#   y_data_list.append(y_train)\n",
        "\n",
        "# print(len(y_data_list))\n",
        "# print(len(data_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s9MO4EO4iw2E"
      },
      "source": [
        "### Feature Extracting Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "QRNn-ctriw2F"
      },
      "outputs": [],
      "source": [
        "# y_test_list = []\n",
        "# data_test_list = []\n",
        "# for x_test, y_test in test_dataloader:\n",
        "#   x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "\n",
        "#   features = feature_extractor(x_test)\n",
        "#   features = minmax_scaler(features.cpu().numpy())\n",
        "#   features = torch.from_numpy(features)\n",
        "#   data_test_list.append(features)\n",
        "\n",
        "#   y_test_list.append(y_test)\n",
        "\n",
        "# print(len(y_test_list))\n",
        "# print(len(data_test_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fO5ySct2iw2F"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "4iVjhjhfiw2F"
      },
      "outputs": [],
      "source": [
        "num_rows = 4\n",
        "num_cols = 3\n",
        "max_neighborhood_range = 1\n",
        "max_learning_rate = 0.01\n",
        "max_steps = 20\n",
        "is_2d_10_neuron = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "som_init_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True )\n",
        "num_features = 512\n",
        "\n",
        "def initial_som():\n",
        "\n",
        "  unique_labels = list(set(train_data.targets))\n",
        "  random.shuffle(unique_labels)\n",
        "  selected_samples = []\n",
        "\n",
        "  # Iterate over the train_dataloader until we have selected enough samples\n",
        "  for x_train, y_train in som_init_dataloader:\n",
        "      x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "      feature = feature_extractor(x_train).cpu().numpy()\n",
        "      \n",
        "\n",
        "      # Check if the label is one of the unique labels and has not been selected already\n",
        "      if y_train[0] not in [sample[1] for sample in selected_samples]:\n",
        "        selected_samples.append(feature[0])\n",
        "\n",
        "      # Break the outer loop if we have collected enough samples\n",
        "      if len(selected_samples) == num_rows * num_cols:\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize SOM with random values\n",
        "  np.random.seed(40)\n",
        "  som = np.random.random_sample(size=(num_rows, num_cols, num_features)) # map construction\n",
        "\n",
        "  i =0\n",
        "  for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "      if not is_2d_10_neuron:\n",
        "        som[row][col] = selected_samples[i]\n",
        "        i+=1\n",
        "      else:\n",
        "        if (row == 3 and col == 0) or (row == 3 and col == 2):\n",
        "          som[row][col] = 10e9\n",
        "        else:\n",
        "          som[row][col] = selected_samples[i]\n",
        "          i+=1\n",
        "        som[3][0] = 10e9\n",
        "        som[3][2] = 10e9\n",
        "  return som\n",
        "\n",
        "\n",
        "som = initial_som()\n",
        "som = torch.from_numpy(som).to(device)\n",
        "if is_2d_10_neuron:\n",
        "  pass\n",
        "  \n",
        "\n",
        "\n",
        "# print(som.shape)\n",
        "# for row in range(num_rows):\n",
        "#     for col in range(num_cols):\n",
        "#       if som[row][col] == None:\n",
        "#         print(f'{row}, {col} nan')\n",
        "#       else:\n",
        "#         print(f'{row}, {col}')\n",
        "# print(som[3][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQt50DvLdxbZ",
        "outputId": "16d0824f-962e-46f6-c0e1-58869913629a"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Pk1e4uJniw2G"
      },
      "source": [
        "## Initializing Self Organising Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "IgwcZFX5iw2G"
      },
      "outputs": [],
      "source": [
        "# num_features = 512 # numnber of dimensions in the input data\n",
        "\n",
        "# if is_2d_10_neuron:\n",
        "#   np.random.seed(40)\n",
        "#   som = np.random.random_sample(size=(num_rows, num_cols, num_features)) # map construction\n",
        "#   som[3][0] = None\n",
        "#   som[3][2] = None\n",
        "#   som = torch.from_numpy(som).to(device)\n",
        "\n",
        "# else:\n",
        "#   np.random.seed(40)\n",
        "#   som = np.random.random_sample(size=(num_rows, num_cols, num_features)) # map construction\n",
        "#   som = torch.from_numpy(som).to(device)\n",
        "\n",
        "# print(som.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG1HwB9Iiw2G",
        "outputId": "03ba1a18-288a-48d7-e1f2-3ad73db1574e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   0%|\u001b[34m          \u001b[0m| 0/50000 [00:00<?, ?it/s]<ipython-input-251-39060b991045>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  final_features = (torch.cat([final_features] + [torch.tensor(f).view(1, -1) for f in features])).detach().clone()\n",
            "Epoch 1:  44%|\u001b[34m████▎     \u001b[0m| 21780/50000 [03:52<05:15, 89.38it/s]"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "\n",
        "\n",
        "\n",
        "scheduler = CosineScheduler(max_update=epochs, base_lr=max_learning_rate, final_lr=0.0001)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    map = np.empty(shape=(num_rows, num_cols), dtype=object)\n",
        "    for row in range(num_rows):\n",
        "      for col in range(num_cols):\n",
        "        map[row][col] = []\n",
        "    final_features = torch.zeros(0,dtype=torch.long, device=device)\n",
        "    final_y = torch.zeros(0,dtype=torch.long, device=device)\n",
        "    bmus = []\n",
        "    for x_train, y_train in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", colour=\"blue\"):\n",
        "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "        features = feature_extractor(x_train)\n",
        "        # final_features = features\n",
        "        # final_y = y_train\n",
        "        label_data = y_train.cpu().numpy()\n",
        "        \n",
        "        neighbourhood_range = neighborhood_optimizer(epoch, epochs, max_neighborhood_range)\n",
        "        learning_rate=scheduler(epoch)\n",
        "        \n",
        "        # if epoch == epochs-1:\n",
        "        final_features = (torch.cat([final_features] + [torch.tensor(f).view(1, -1) for f in features])).detach().clone()\n",
        "        final_y = torch.cat([final_y,y_train.view(-1)])\n",
        "\n",
        "        # start training iterations\n",
        "        for i in range(features.shape[0]):\n",
        "          bmu = bmu_search(features[i], som, num_rows, num_cols)\n",
        "          map[bmu[0]][bmu[1]].append(label_data[i])\n",
        "          bmus.append(bmu)\n",
        "          for row in range(num_rows):\n",
        "            for col in range(num_cols):\n",
        "              if is_2d_10_neuron and (row == 3 and col ==0 or row ==3 and col ==2):\n",
        "                pass\n",
        "              else:\n",
        "                dist = manhattan_distance([row, col], bmu)\n",
        "                if dist <= neighbourhood_range:\n",
        "                  som[row][col] += learning_rate * distance_func(dist) * (features[i].to(device) - som[row][col].to(device)) #update neighbour's weight\n",
        "\n",
        "    label_map = np.zeros(shape=(num_rows, num_cols),dtype=np.int64)\n",
        "    label_dispersion = np.zeros(shape=(num_rows, num_cols), dtype=np.float64)\n",
        "    for row in range(num_rows):\n",
        "      for col in range(num_cols):\n",
        "        label_list = map[row][col]\n",
        "        if len(label_list)==0:\n",
        "          label = -3\n",
        "          dispersion = 0.0\n",
        "        else:\n",
        "          label = max(label_list, key=label_list.count)\n",
        "          count_label = label_list.count(label)\n",
        "          count_all_labels = len(label_list)\n",
        "          dispersion = count_label / count_all_labels\n",
        "\n",
        "        label_map[row][col] = label\n",
        "        label_dispersion[row][col] = dispersion \n",
        "    labels_from_bmu = np.zeros(shape=len(bmus),dtype=np.int64)\n",
        "    for i, bmu in enumerate(bmus):\n",
        "      labels_from_bmu[i] = label_map[bmu[0]][bmu[1]]\n",
        "    acc = accuracy_score(final_y.cpu().numpy(), labels_from_bmu)\n",
        "    f1 = f1_score(final_y.cpu().numpy(), labels_from_bmu, average='weighted')\n",
        "    print(\"Accuracy: \", acc)\n",
        "    print(\"F1 score:\", f1)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKUcQbDtUJIC"
      },
      "outputs": [],
      "source": [
        "print(final_features.shape)\n",
        "print(final_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IhfVcu2aiw2H"
      },
      "source": [
        "## Collecting Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3iQNRCUiw2H"
      },
      "outputs": [],
      "source": [
        "# map = np.empty(shape=(num_rows, num_cols), dtype=object)\n",
        "# # final_features_np = final_features.cpu().numpy()\n",
        "# # final_y_np = final_y.cpu().numpy()\n",
        "# # print(final_features_np.shape[0])\n",
        "# for row in range(num_rows):\n",
        "#   for col in range(num_cols):\n",
        "#     if som[row][col] != None:\n",
        "#       map[row][col] = [] # empty list to store the label\n",
        "\n",
        "# # for i, features in enumerate(final_features_np):\n",
        "\n",
        "# label_data = final_y.cpu().numpy()\n",
        "\n",
        "# for t in range(final_features.shape[0]):\n",
        "  \n",
        "#   bmu = bmu_search(final_features[t].to(device), som.to(device), num_rows, num_cols)\n",
        "#   # print(bmu)\n",
        "#   map[bmu[0]][bmu[1]].append(label_data[t]) # label of winning neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f8zWNs1Qiw2H"
      },
      "source": [
        "## Construct Label Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8kQrpssiw2I"
      },
      "outputs": [],
      "source": [
        "# label_map = np.zeros(shape=(num_rows, num_cols),dtype=np.int64)\n",
        "\n",
        "# label_dispersion = np.zeros(shape=(num_rows, num_cols), dtype=np.float64)\n",
        "# for row in range(num_rows):\n",
        "#   for col in range(num_cols):\n",
        "#     if som[row][col] != None:\n",
        "#       label_list = map[row][col]\n",
        "#       if len(label_list)==0:\n",
        "#         label = -3\n",
        "#         dispersion = 0.0\n",
        "#       else:\n",
        "#         label = max(label_list, key=label_list.count)\n",
        "#         count_label = label_list.count(label)\n",
        "#         count_all_labels = len(label_list)\n",
        "#         dispersion = count_label / count_all_labels\n",
        "\n",
        "#       label_map[row][col] = label\n",
        "#       label_dispersion[row][col] = dispersion\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(som[3][0])"
      ],
      "metadata": {
        "id": "aD_OM8OE_Yhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr7SkygYmpP4"
      },
      "outputs": [],
      "source": [
        "n=0\n",
        "total = 0\n",
        "for row in range(num_rows):\n",
        "  for col in range(num_cols):\n",
        "    total +=label_dispersion[row][col]\n",
        "    n += 1\n",
        "\n",
        "avg_dispersion = total/n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "t-lr11zYiw2I"
      },
      "source": [
        "## Feature Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HQ3msvfiw2I"
      },
      "outputs": [],
      "source": [
        "title = ('Feature Map')\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.imshow(label_map, cmap='Blues')\n",
        "ax.set_xticks(np.arange(num_cols))\n",
        "ax.set_yticks(np.arange(num_rows))\n",
        "ax.set_xticklabels(np.arange(1, num_cols+1))\n",
        "ax.set_yticklabels(np.arange(1, num_rows+1))\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "for i in range(num_rows):\n",
        "  for j in range(num_cols):\n",
        "    text = ax.text(j, i, '{:.2f}'.format(label_map[i][j]),\n",
        "                   ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "plt.colorbar()\n",
        "plt.title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAR1CdRBjegT"
      },
      "source": [
        "### dispersion map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO-Uld8rjCZd"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "im = ax.imshow(label_dispersion, cmap='Blues')\n",
        "ax.set_xticks(np.arange(num_cols))\n",
        "ax.set_yticks(np.arange(num_rows))\n",
        "ax.set_xticklabels(np.arange(1, num_cols+1))\n",
        "ax.set_yticklabels(np.arange(1, num_rows+1))\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "for i in range(num_rows):\n",
        "  for j in range(num_cols):\n",
        "    text = ax.text(j, i, '{:.2f}'.format(label_dispersion[i][j]),\n",
        "                   ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "plt.title('Label Dispersion Map')\n",
        "plt.colorbar(im)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJawfY2Zjjnb"
      },
      "source": [
        "### Scatterplot of dispersion for each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsVZgPr_jasj"
      },
      "outputs": [],
      "source": [
        "# for i in range(num_rows):\n",
        "#   for j in range(num_cols):\n",
        "    \n",
        "#     y = map[i][j]\n",
        "#     # x = range(len(y))\n",
        "#     print(label_map[i][j])\n",
        "#     colors = ['green' if k == label_map[i][j] else 'blue' for k in y]\n",
        "#     plt.hist(y, bins=11, c=colors)\n",
        "#     # plt.scatter(x, y, c=colors)\n",
        "#     # plt.xlabel('Index')\n",
        "#     # plt.ylabel('Y values')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Xp1Kx1gUiw2I"
      },
      "source": [
        "## Test Data\n",
        "using the trained som, search the winning node of corresponding to the test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GQ0X5OMiw2J"
      },
      "outputs": [],
      "source": [
        "sum_acc = 0\n",
        "sum_f1 = 0\n",
        "n = 0\n",
        "for x_test, y_test in test_dataloader:\n",
        "  x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "  features = feature_extractor(x_test)\n",
        "\n",
        "  winner_labels = []\n",
        "\n",
        "  for t in range(features.shape[0]):\n",
        "    bmu = bmu_search(features[t], som, num_rows, num_cols)\n",
        "    row = bmu[0]\n",
        "    col = bmu[1]\n",
        "    predicted = label_map[row][col]\n",
        "    winner_labels.append(predicted)\n",
        "  acc = accuracy_score(y_test.cpu().numpy(), winner_labels)\n",
        "  f1 = f1_score(y_test.cpu().numpy(), winner_labels, average='weighted')\n",
        "  sum_f1 += f1\n",
        "  sum_acc += acc\n",
        "  n += 1\n",
        "  # print(\"Accuracy: \",acc)\n",
        "\n",
        "print(\"Total Accuracy: \", sum_acc /n)\n",
        "print(\"Average F1 score:\", sum_f1 / n)\n",
        "print(\"Average Dispersion :\", avg_dispersion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMYWX0fKMnE0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}